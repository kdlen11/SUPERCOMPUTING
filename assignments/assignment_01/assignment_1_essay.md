I have two motivations for learning high-performing computing. The first stems from my lab work with Professor Ford in his Ford AI and Society (FAIS) Lab, where I utilize his Digital Twin technology. The Digital Twins are housed primarily on the William & Mary HPC systems, and for this reason the more I learn about high-performance computing, the more efficient and functional my code for the FAIS lab will be. My second motivation for taking this course is simply to obtain an increased knowledge of HPC systems which I can take with me into my career as a Data Scientist after I graduate in May — in a world of ever-increasing dataset sizes and research complexity, it is a valuable domain to be familiar with.
My goal for the course, as I have said in class, is to formalize my understanding of HPC which I have obtained in my lab work. The knowledge that I already have was gained by delving straight into my research in a trial-by-fire fashion, and while I generally comprehend the basics of HPC which are necessary for my lab work, I have spent little time exploring exactly how these systems function or best practices for their usage. My hope is that by slowly building my foundation from that of a beginner in this course, I pick up on some HPC information I have (most surely) missed along the way. My plan of attack is to approach this course as I would any other Data Science course at William & Mary — to get my work done in advance, pay attention in class, ask lots of questions, and finally to be open and receptive to collaborative processes. 
The folder structure that I have created is good for reproducible research for a number of reasons. The folder structure is simple and comprehensible — there are no long or confusing names, there are no overlapping or redundant folders, and the structure is not too deep (that is, there are no series of deeply nested folders). This means that upon reproduction, all of the data and files in any stage of any research project using this folder structure are accessible. Understandable, and recreatable.
To my understanding, we use documented code in our computational research tasks for the sake of clarity. Code which is well documented, whether that be via README files or comments within scripts themselves, is easy to understand — which is paramount when fresh eyes look at existing code, whether that be fellow researchers, superiors, or even the original programmer, months or years later. Additionally, well-documented code makes it easier to find possible bugs or errors, and in the process of documenting code one can sometimes catch and avoid them entirely. Finally, it is necessary for code to be well documented for the sake of accountability and scrutiny from other researchers: it is in this manner that scientific progress emerges.
I did have one question about the course: I did not see it mentioned explicitly on the syllabus, but I am curious if we are going to cover containerization using W&M HPC resources at all in our course, such as Kubernetes or Docker?
